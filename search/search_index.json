{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Source AI Workshop","text":""},{"location":"#open-source-ai-workshop","title":"Open Source AI Workshop","text":"<p>You've probably heard how tools like ChatGPT are changing workflows \u2014 but when it comes to privacy, security, and control, using public AI tools isn't always an option. In this hands-on workshop, you'll learn how to run your own local, open-source LLMs \u2014 no cloud, no cost, and no compromise.</p> <p>We'll walk through installing and running models with tools like ollama, AnythingLLM, and Continue using familiar environments like VS Code. By the end, you'll have a fully functional local AI assistant, ready to support your work securely and offline.</p> <p>Our overarching goals of this workshop is as follows:</p> <ul> <li>Learn about Open Source AI and its general use cases.</li> <li>Use an open source LLM that is built in a verifiable and legal way.</li> <li>Learn about Prompt Engineering and how to leverage a local LLM in daily tasks.</li> </ul> <p>Tip</p> <p>Working with AI is all about exploration and hands-on engagement. These labs are designed to give you everything you need to get started \u2014 so you can collaborate, experiment, and learn together. Don\u2019t hesitate to ask questions, raise your hand, and connect with other participants.</p>"},{"location":"#agenda","title":"Agenda","text":"Lab Description Lab 0: Workshop Pre-work Install pre-requisites for the workshop Lab 1: Configuring Open-WebUI Set up Open-WebUI to start using an LLM locally Lab 2: Chatting with Your Local AI Get acquainted with your local LLM Lab 3: Prompt Engineering Learn about prompt engineering techniques Lab 4: Applying What You Learned Refine your prompting skills Lab 5: Using Open-WebUI for a local RAG Write code using Continue and Granite Lab 7: Using Mellea to help with Generative Computing Learn how to leverage Mellea for Advanced AI situations <p>Thank you SO MUCH for joining us in this workshop! If you have any questions or feedback, the TAs would love answer them for you. If you come across any issues or bugs, don't hesitate to open a Pull Request or an Issue -- we'll take a look as soon as we can.</p>"},{"location":"#compatibility","title":"Compatibility","text":"<p>This workshop has been tested on the following platforms:</p> <ul> <li>MacOS: version 14.5</li> <li>Linux: Fedora 40</li> <li>Windows 11</li> </ul>"},{"location":"#credits","title":"Credits","text":"<ul> <li>JJ Asghar</li> <li>Gabe Goodhart</li> <li>Ming Zhao</li> <li>Rafael Vasquez</li> </ul>"},{"location":"lab-1/","title":"Configuring AnythingLLM","text":""},{"location":"lab-1/#setup","title":"Setup","text":"<p>Let's start by configuring AnythingLLM installed and <code>ollama</code> to talk to one another. The following screenshots will be from a Mac, but this should be similar on Windows and Linux.</p> <p>First, if you haven't already, download the Granite 4 model. Make sure that <code>ollama</code> is running in the background (you may have to run <code>ollama serve</code> in its own terminal depending on how you installed it) and in another terminal run the following command:</p> <pre><code>ollama pull granite4:micro\n</code></pre> <p>Note</p> <p>The download may take a few minutes depending on your internet connection. In the meantime, you can check out information about model we're using here. Check out how many languages it supports and take note of its capabilities. It'll help you decide what tasks you might want to use it for in the future.</p> <p>Open the AnythingLLM desktop application and either click on the Get Started button or open up settings (the \ud83d\udd27 button). For now, we are going to configure the global settings for <code>ollama</code> but you can always change it in the future.</p> <p></p> <p>Click on the LLM section, and select Ollama as the LLM Provider. Select the <code>granite4:micro</code> model you downloaded. You'd be able to see all the models you have access to through <code>ollama</code> here.</p> <p></p> <p>Click the Back to workspaces button (where the \ud83d\udd27 was) and head back to the homepage.</p> <p>Click New Workspace.</p> <p></p> <p>Give it a name (e.g. the event you're attending today):</p> <p></p>"},{"location":"lab-1/#testing-the-connection","title":"Testing the Connection","text":"<p>Now, let's test our connection AnythingLLM! I like asking the question, \"Who is Batman?\" as a sanity check. Every LLM should know who Batman is.</p> <p>The first response may take a minute to process. This is because <code>ollama</code> is spinning up to serve the model. Subsequent responses should be much faster.</p> <p></p> <p>You may notice that your answer is slightly different then the screen shot above. This is expected and nothing to worry about!</p>"},{"location":"lab-1/#conclusion","title":"Conclusion","text":"<p>Congratulations! Now you have AnythingLLM running and it's configured to work with <code>granite4:micro</code> and <code>ollama</code>. Move on to Lab 2 and have a chat with your model!</p>"},{"location":"lab-1.5/","title":"Configuring Open-WebUI","text":""},{"location":"lab-1.5/#setup","title":"Setup","text":"<p>Let's start by configuring Open-WebUI and <code>ollama</code> to talk to one another. The following screenshots will be from a Mac, but this should be similar on Windows and Linux.</p> <p>First, if you haven't already, download the Granite 4 model. Make sure that <code>ollama</code> is running in the background (you may have to run <code>ollama serve</code> in its own terminal depending on how you installed it) and in another terminal run the following command:</p> <pre><code>ollama pull granite4:micro\n</code></pre> <p>Note</p> <p>The download may take a few minutes depending on your internet connection. In the meantime, you can check out information about model we're using here. Check out how many languages it supports and take note of its capabilities. It'll help you decide what tasks you might want to use it for in the future.</p> <p>Open up Open-WebUI (assuming you've run <code>open-webui serve</code>): by using this URL with your browser: http://localhost:8080/</p> <p></p> <p>If you see something similar, Open-WebUI is installed correctly! Continue on, if not, please find a workshop TA or raise your hand for some help.</p> <p>Click Getting Started. Fill out the next screen and click the Create Admin Account. This will be your login for your local machine. Remember that this because it will be your Open-WebUI configuration login information if want to dig deeper into it after this workshop.</p> <p>You should see the Open-WebUI main page now, with <code>granite4:micro</code> right there in the center!</p> <p></p>"},{"location":"lab-1.5/#testing-the-connection","title":"Testing the Connection","text":"<p>Test it out! I like asking the question, \"Who is Batman?\" as a sanity check. Every LLM should know who Batman is.</p> <p>The first response may take a minute to process. This is because <code>ollama</code> is spinning up to serve the model. Subsequent responses should be much faster.</p> <p></p> <p>You may notice that your answer is slightly different then the screen shot above. This is expected and nothing to worry about!</p>"},{"location":"lab-1.5/#conclusion","title":"Conclusion","text":"<p>Congratulations! Now you have Open-WebUI running and it's configured to work with <code>granite4:micro</code> and <code>ollama</code>. Move on to Lab 2 and have a chat with your model!</p>"},{"location":"lab-2/","title":"Chatting with Your Local AI","text":"<p>It's time for the fun exploration part your Prompt Engineering (PE) journey. In this lab, you're encouraged to spend as much time as you can chatting with the model, especially if you have little experience doing so. Keep some questions in mind: can you make it speak in a different tone? Can it provide a recipe for a cake or a poem about technology? Is it self-aware?</p>"},{"location":"lab-2/#chatting-with-the-model","title":"Chatting with the Model","text":"<p>Open a brand new Workspace in AnythingLLM (or Open-WebUI) called \"Learning Prompt Engineering\".</p> <p>You can pretty much write whatever you want to, give it a shot. Treat it like a good friend, be polite, and guide it to an answer you are looking for.</p> <p>For some inspiration, I like to start with <code>Who is Batman?</code> then work from there. I might want to see if the LLM knows who Batman's top 10 enemies are, or what was the most creative way Batman saved the day? Some example responses to those questions are below.</p> <p>!! note     If you treat the LLM like a knowledge repository, you can get a lot of useful information out of it. But, remember not to     blindly accept its output. You should always cross-reference important things. Treat it like a confident librarian! They've read     a lot and they can be very fast at finding books, but they can mix things up too!</p>"},{"location":"lab-2/#using-the-ollama-cli","title":"Using the <code>ollama</code> CLI","text":"<p>This is an example of of using the CLI with vanilla ollama:</p> <p>First, use ollama to list the models that you currently have downloaded: <pre><code>ollama list\n</code></pre> And you'll see a list similar to the following: <pre><code>ollama list\nNAME                         ID              SIZE      MODIFIED           \ngranite4:micro               4235724a127c    2.1 GB    23 hours ago \n</code></pre> Next, use Ollama to run one of the models:</p> <p><pre><code>ollama run granite4:micro\n</code></pre> And ask it questions, like this: <pre><code>Who is Batman?\n</code></pre> And it returns something like this: <pre><code>&gt;&gt;&gt; Who is Batman?\nBatman is a fictional superhero created by artist Bob Kane and writer Bill Finger. He first appeared in Detective Comics #27,\npublished by DC Comics in 1939. Born as Bruce Wayne, he becomes Batman to fight crime after witnessing the murder of his parents\nas a child. Unlike most superheroes, he has no actual superpowers but instead relies on his physical prowess, intellect, and\nadvanced technology. His symbol is a bat, which instills fear in criminals due to its nocturnal nature. Batman is known for his\nstrong moral code, not killing his enemies, and his relentless pursuit of justice. He's one of the most iconic and enduring\ncharacters in the world of comics and popular culture.\n</code></pre></p> <pre><code>&gt;&gt;&gt; Who were Batman's top 10 enemies?\nBatman has faced numerous villains over the years, but here are ten of his most notable adversaries:\n\n1. The Joker - One of Batman's archenemies, The Joker is a criminal mastermind known for his chaotic and psychopathic behavior.\nHe often uses deadly weapons disguised as everyday objects like flowers or toys.\n\n2. Two-Face (Harvey Dent) - Once Batman's ally and Gotham City district attorney, Harvey Dent was scarred by acid and became a\nvillain with a coin that decides his actions, representing the duality of good and evil within him.\n\n3. Penguin (Oswald Cobblepot) - A short, stout criminal with an umbrella-based arsenal, Penguin is known for his love of\nanimals, especially birds, and operates from his hideout in the sewers beneath Gotham City.\n\n4. Catwoman (Selina Kyle) - A skilled thief and Batman's complex love interest, Catwoman wears a cat-themed costume and uses her\nagility, charm, and whip to outwit her adversaries.\n\n5. Ra's al Ghul - An immortal villain who leads the League of Assassins, Ra's seeks to cleanse the world with natural disasters\nand replace humanity with a more \"pure\" species. Batman's ally Talia heads the organization.\n\n6. The Riddler (Edward Nygma) - A criminal genius with an obsession for riddles, The Riddler uses complex puzzles to taunt\nBatman while committing crimes.\n\n7. Mr. Freeze (Victor Fries) - Once a brilliant cryogenic scientist, Victor Fries became Mr. Freeze after an accident involving\ncryogenic technology left him needing a refrigerated suit to survive.\n\n8. Poison Ivy (Dr. Pamela Isley) - A plant-based villainess with control over plants and the ability to manipulate pheromones,\nshe seeks to destroy humanity and create a new world dominated by flora.\n\n9. Bane - A supervillain known for his physical strength, enhanced by a drug called Venom, Bane once broke Batman's back in a\nlegendary storyline.\n\n10. Ra's al Ghul's Daughter Talia - As mentioned earlier, Talia is both the daughter of Ra's and an occasional love interest for\nBruce Wayne/Batman. She has played both roles as ally and enemy throughout various Batman stories.\n\nThese villains represent a mix of criminal masterminds, twisted psychopaths, and formidable physical threats that challenge\nBatman's intellect, strength, and resolve.\n</code></pre> <pre><code>&gt;&gt;&gt; What was the most interesting way Batman saved the day?\nOne of the most intriguing and innovative ways Batman has saved the day is from Frank Miller's seminal graphic novel \"The Dark\nKnight Returns.\" In this story, an aging Batman returns to crime-fighting after a self-imposed retirement.\n\nAfter a series of escalating conflicts with a new mutant gang known as the Mutants, led by Green Arrow's former sidekick, Talia\nal Ghul (daughter of Ra's al Ghul), Batman faces off against their leader, a mutated Joker. The Joker has been given superhuman\nstrength through a risky experiment conducted by his henchmen.\n\nThe climax of the storyline takes place in a nuke-laden Bomb Squad truck, driven by the Joker himself and escorted by the\nMutants. With conventional methods proving ineffective against the Joker's newfound power, Batman devises an unconventional\nplan:\n\n1. Luring the Joker into a trap at the Gotham Bay.\n2. Tying a massive concrete block to his leg and dropping him into the water, hoping the weight would neutralize the mutagenic\nserum, returning the Joker's strength to normal levels.\n3. Simultaneously triggering an explosion that would capsize the Bomb Squad truck carrying the nuclear bomb, causing it to sink\nwith the now-weakened Joker still inside.\n4. Jumping into the water after the Joker and subduing him just before the truck goes underwater.\n5. Finally, using a remote detonator to destroy the truck and the bomb, thus saving Gotham City from a catastrophic explosion.\n\nThis daring plan showcases Batman's resourcefulness, unwavering determination, and willingness to take risks for the greater\ngood - all hallmarks of his character. The innovative approach to saving the day has since become one of the most iconic moments\nin Batman's extensive history.\n</code></pre>"},{"location":"lab-2/#conclusion","title":"Conclusion","text":"<p>Spend as much time as you want asking your LLM about anything about any topic and exploring how you can alter its output to provide you with more interesting or satisfying responses.</p> <p>When you are acquainted with your model, move on to Lab 3 to learn about Prompt Engineering.</p>"},{"location":"lab-3/","title":"Prompt Engineering","text":"<p>Note</p> <p>This lab was informed by @juliandiscovers and @BenzaMaman's workshops and content and presents a broad overview of what they teach. If you want to dive deeper into Prompt Engineering we recommend checking them out.</p>"},{"location":"lab-3/#what-is-prompt-engineering-pe","title":"What is Prompt Engineering (PE)?","text":"<p>Prompt engineering is the practice of designing clear, intentional instructions to guide the behavior of an AI model.</p> <p>It involves crafting prompts that help a model identify what task to perform, how to perform it, and if there are considerations in style or format. This can include specifying tone, structure, context, or even assigning the AI a particular role.</p> <p>Prompt engineering is essential because the quality and precision of the prompt can significantly influence the quality, relevance, and creativity of the generated output. As generative models become more powerful, skillful prompting becomes a key tool for unlocking their full potential.</p>"},{"location":"lab-3/#the-three-key-principles-of-pe","title":"The Three Key Principles of PE","text":"<ol> <li>Specificity - The more detailed your criteria, the more focused and relevant the output will be.</li> <li>Step-by-step structuring - Break complex tasks down into smaller parts to guide the model more effectively.</li> <li>Iterative refinement - Adjust your prompt or build on previous responses to achieve the best result.</li> </ol>"},{"location":"lab-3/#what-makes-a-good-prompt","title":"What Makes a Good Prompt?","text":"<ol> <li>Use natural, unambiguous language \u2013 Phrase your prompt as if you're explaining the task to a person.</li> <li>Provide guiding context \u2013 Share relevant background information, examples, or constraints to clarify your intent.</li> <li>Define format and tone when needed \u2013 Specify style, structure, or voice (e.g., bullet points, formal, pirate).</li> <li>Adapt based on responses \u2013 Treat the model\u2019s output as feedback and refine accordingly.</li> </ol>"},{"location":"lab-3/#the-prompting-workflow","title":"The Prompting Workflow","text":"<ol> <li>Define the problem or goal \u2192 \u201cI want a fun fact about cats to share with kids.\u201d</li> <li>Include relevant context and keywords \u2192 \u201cKeep it short, easy to understand, and fun.\u201d</li> <li>Write the prompt \u2192 \u201cTell me a fun and simple fact about cats that a 7-year-old would enjoy.\u201d</li> <li>Test, evaluate, and iterate \u2192 If the fact is too complex: \u201cMake it even simpler and add a playful tone.\u201d</li> </ol>"},{"location":"lab-3/#types-of-prompts","title":"Types of Prompts","text":"<p>Open a brand new Workspace in AnythingLLM (or Open-WebUI) called \"Learning Prompt Engineering\". Read along with the examples below, and be sure to try them out for yourself!</p>"},{"location":"lab-3/#zero-shot-prompting","title":"Zero-shot Prompting","text":"<p>These prompts don't have any previous data, structure, or guidelines provided with the request. Here's an example:</p> <pre><code>I want to explore pasta making recipes.\nDo you have any suggestions for recipes that are unique and challenging?\n</code></pre> <p>As you can see, this Granite model comes back with some very challenging options:</p> <p></p> <p>Try it for yourself, did you get a different response? Think about how the response could be better.</p> <p>As a follow-up, I'll ask for the recipe to make the \"Homemade Ravioli\" option in the response I received:</p> <pre><code>I do like some homemade ravioli.\nWhat is the spinach, ricotta and cheese recipe you suggest?\n</code></pre> <p></p> <p>These simple back-and-forth questions are examples of **zero-shot prompts*.</p> <p>Come up with your own zero-shot prompt about any subject you want. Then, we'll start to add some complexity to our prompts.</p>"},{"location":"lab-3/#one-shot-and-multi-shot-prompting","title":"One-Shot and Multi-Shot Prompting","text":"<p>First, create a new \"thread\" so the context window resets. You can think of a context window as the amount of information a model can \"remember\".</p> <p></p> <p>In the following examples, we'll add more guidance in our prompt. By providing one example or structure, we achieve one-shot prompting.</p> <p>Take the provided prompts, and replace the [words] in brackets with your own choices. Get creative with it!</p> <pre><code>I want you to act as a customer support assistant who is [characteristic].\nHow would you respond to [text] as a representative of our [type] company?\n</code></pre> <p>My version will be: <pre><code>I want you to act as a customer support assistant who is an expert in shipping logistics.\nHow would you respond to client who has had their freight lost as a representative of our company?\n</code></pre></p> <p></p> <p>That's not a satisfactory or interesting response, right? We need to iterate on it, and provide more context about the client, like what they may have lost. Tip: always think about adding more context!</p> <pre><code>The freight they lost was an industrial refrigerator, from Burbank, California to Kanas City, MO.\nI need you to write out an apology letter, with reference to the shipping order #00234273\nand the help line of 1-800-347-2845, with a discount code of OPPSWEDIDITAGAIN for 15% off\nshipping their next order. Mention that sometimes, the trucks have accidents and need\nto be repaired and we should be able to reach out in a couple weeks.\n</code></pre> <p></p> <p>So much better! By providing more context and more insight into what you are expecting in a response, we can improve the quality of our responses greatly. Also, by providing multiple examples, you're achieving multi-shot prompting!.</p>"},{"location":"lab-3/#conclusion","title":"Conclusion","text":"<p>Now that you know the basics of prompt engineering and simple techniques you can use to level-up your prompts, let's move on to Lab 4 and apply what you've learned with some exercises.</p>"},{"location":"lab-4/","title":"Applying What You Learned","text":"<p>Complete the following exercises using your local LLM. Try to come up with your own prompts from scratch! Take note of what works and what doesn't.</p> <ul> <li>Be curious! What if you ask the same question but in a different way, does the response significantly change?</li> <li>Be creative! Do you want the response to be organized in a numbered or bulleted list instead of sentences?</li> <li>Be specific! Aim for perfection. Use descriptive language and examples to perfect your output.</li> </ul> <p>Note</p> <p>Discovered something cool or unexpected? Don\u2019t keep it to yourself, raise your hand or let the TA know! Whether it\u2019s a creative use of the tool or a surprising result, we\u2019d love to hear it. Past students have used granite to run D&amp;D one-shots and co-write stories, your idea might inspire the whole room!</p>"},{"location":"lab-4/#1-generate-a-creative-story","title":"1. Generate a Creative Story","text":"<p>Ask the model to generate three creative sentences about camels.</p> <p>Each sentence must:</p> <ul> <li>Be written from a different point of view (e.g., a child, a scientist, or the camel itself),</li> <li>Include at least one vivid sensory detail (sight, sound, touch, etc.),</li> <li>And build a mini narrative across the three (the second sentence should follow from the first, the third from the second).</li> </ul> <p>Format the sentences like a list.</p>"},{"location":"lab-4/#2-extract-summary-topics","title":"2. Extract Summary Topics","text":"<p>Prompt a model to extract a short, bulleted summary from a meeting transcript. The goal is to turn raw dialogue into clear, actionable notes \u2014 just like you'd want after a real meeting.</p> <p>You can use the following meeting transcripts.</p> <pre><code>00:00 [joe] The goal today is to finalize the budget proposal.\n00:15 [sarah] I think we should allocate 30% to marketing.\n00:30 [joe] That's a good starting point.\n00:50 [dave] I agree, but we should also consider allocating 20% to research and development.\n01:15 [sarah] That's a good point. What about the remaining 50%?\n01:40 [dave] We could use it for operational costs.\n02:05 [joe] I think that's a solid plan. Are we all in agreement?\n02:30 [sarah] Yes, I am.\n02:50 [dave] Me too.\n03:15 [joe] Done!\n</code></pre> <pre><code>00:00 [jess] Let's plan the company picnic!\n00:10 [ali] How about we have it at the beach?\n00:21 [sarah] Good idea.\n00:35 [ali] Can we have a BBQ and games too?\n00:50 [jess] Maybe we could also have a potluck?\n01:05 [sarah] That way everyone can bring something to share.\n01:20 [ali] We could also set up a few outdoor activities like volleyball and badminton.\n01:40 [jess] I like this plan. Let's make it happen!\n</code></pre> <p>Here's an example summary:</p> <pre><code>Transcript:\n00:00 [jane] What's the status on Project A?\n00:08 [bob] Oh, before we dive in\u2014did everyone have a good weekend?\n00:12 [jane] Haha, yeah, it went by too fast. Okay, back to it\u2014Project A?\n00:15 [bob] We're still waiting on the Johnson report for Project B, which is kind of blocking progress.\n00:30 [jane] Got it. Can we get an update on the timeline for Project C?\n00:37 [bob] Hmm, let me check... one sec...\n00:45 [bob] Okay, I\u2019ll send out an email with the latest information right after this.\n01:02 [jane] Sounds good. Also, let's schedule a meeting with the marketing team to discuss the campaign for Project D.\n01:10 [bob] Marketing\u2026 got it. Give me a second to pull up the calendar.\n01:18 [bob] I\u2019ll set it up for next Thursday.\n01:25 [jane] Perfect. Thanks.\n01:30 [bob] No problem.\n01:35 [jane] Sounds good. Let\u2019s move forward with the plan.\n\nSummary:\n- Project A is pending the Johnson report for Project B\n- Project C update will be sent via email\n- Project D meeting scheduled for next Thursday\n</code></pre>  Show hint  Try prompting like this:  \"Summarize the following meeting transcript as a short bulleted list of key decisions and discussion points.\"  Then paste in the transcript and see what the model gives you. Ask yourself:  - Are the bullets focused and concise? - Did the model include only important information? - Did it ignore filler or small talk?"},{"location":"lab-4/#3-compare-two-passages","title":"3. Compare Two Passages","text":"<p>Prompt the model to analyze and explain what two short passages have in common.</p> <p>Try to generate output that doesn't just focus on shared objects, the model is probably smarter than that! Your prompt should encourage the model to reflect on themes, tone, or emotional meaning, not just surface-level details.</p> <pre><code>The bird sat on the windowsill, its bright feathers a splash of color\nagainst the drab gray wall. It cocked its head to one side, as if listening\nto the distant chirping of its fellow birds. The morning sunlight\nstreamed through the window, casting a warm glow over the bird's\nplumage.\n\nThe bird took to the skies, its wings beating rapidly as it soared above\nthe treetops. The wind ruffled its feathers, but it flew with ease, its eyes\nscanning the landscape below for signs of food or danger. The sun was\nhigh overhead, casting a warm glow over the bird's feathers as it flew\nover the fields and forests.\n</code></pre> <pre><code>It's windy as the sun sets over the horizon, casting a warm glow on the\nlandscape.\n\nA gentle breeze rustles the leaves of the trees as the sun begins to set\nand colour the landscape.\n</code></pre> <pre><code>\"The dog wagged its tail excitedly as it waited for its owner to come\nhome. It had been a long day, and the dog was eager to play and receive\nsome attention. As soon as it heard the door open, the dog ran to greet\nits owner, tail wagging furiously.\"\n\n\"The dog lay on its bed, panting softly as it watched its owner pack\nup the car. It had been a long day, and the dog was tired but content.\nAs soon as its owner gave it a pat and a treat, the dog settled in for a\nwell-deserved nap.\"\n</code></pre> <pre><code>\"The old woman sat on the beach, watching the sun set over the ocean. He\nthought about his life, and the memories he had made with his loved ones.\nAs the stars began to twinkle in the sky, he felt a sense of peace wash over\nhim, knowing that he had lived a full and happy life.\"\n\n\"The old woman sat on the mountain, watching the sun rise over the\nvalley. She thought about her life, and the lessons she had learned from her\nexperiences. As the mist cleared from the valley, she felt a sense of\ncontentment, knowing that she had lived a life of purpose and meaning.\"\n</code></pre>"},{"location":"lab-4/#4-build-a-cover-letter","title":"4. Build a Cover Letter","text":"<p>Ask the model to help write a short, two-paragraph cover letter based on a resume.</p> <p>Use a resume to prompt the model to generate a two-paragraph cover letter that explains:</p> <ol> <li>Who you are and what your expertise is.</li> <li>Why a hiring manager should consider you for a new role.</li> </ol> <p>A sample resume is provided below, but use your own if you have it saved locally! You could even ask the model to generate a short resume for you to use.</p> <pre><code>Name: Jordan Rivera\nCurrent Job: Marketing Specialist\nExperience: 5 years\nCore Skills: content strategy, SEO optimization, team leadership, data analytics\nPast Roles:\n- Social Media Manager at BrightSpark (2 years)\n- Content Writer at BuzzTone Agency (1 year)\nEducation:\n- B.A. in Communications\n</code></pre>  Show example prompt  <pre><code>The following text is my resume for my career up\nuntil my most recent job. I am [current job] with\n[years of experience] considered a highly skilled\nindividual in [core skill set]. I want to build a\ntwo paragraph explanation about why someone should\nhire me for a role with both my current skill set\nand previous experience.\n</code></pre>  ![](../images/anythingllm_resume.png)  The response I received has room for improvement, but gives me something to work with!  <p>Try to build and modify this blurb using the principles of prompt engineering you learned until you're happy with the quality of the response you receive. Think outside of the box!</p>"},{"location":"lab-4/#5-dungeons-dragons-role-play","title":"5. Dungeons &amp; Dragons Role-Play","text":"<p>If you're familiar with the role-playing game Dungeons &amp; Dragons, this excercise is for you! Write a prompt that you would give to an AI to generate a one-shot D&amp;D adventure.</p> <p>Write a prompt that you would give to an AI to generate a one-shot D&amp;D adventure</p> <p>Focus on providing enough detail to create a cohesive story for a one-session gameplay experience. Think about the environment, the objectives, challenges, and the final boss encounter\u2014all while keeping it concise and exciting for the players.</p>  Show example prompt  <pre><code>Generate a self-contained dungeon adventure for a party of 4 adventurers,\nset in a [specific environment like a forgotten temple or an abandoned mine],\nwith a clear objective, unique challenges, and a memorable boss encounter,\nall designed to be completed in a single session of gameplay\n</code></pre>  The best part of this prompt is that you can take the output and extend or shorten the portions it starts with, and tailor the story to your adventurers' needs!"},{"location":"lab-4/#conclusion","title":"Conclusion","text":"<p>Well done! By completing these exercises, you're well on your way to being a prompt expert. In Lab 5, we'll show how to use local RAG with AnythingLLM.</p>"},{"location":"lab-5/","title":"Using AnythingLLM for a local RAG","text":""},{"location":"lab-5/#configuration-and-sanity-check","title":"Configuration and Sanity Check","text":"<p>Open up AnythingLLM, and you should see something like the following: </p> <p>If you see this that means AnythingLLM is installed correctly, and we can continue configuration. If not, please find a workshop TA or raise your hand we'll be there to help you ASAP.</p> <p>Next as a sanity check, run the following command to confirm you have the granite4:micro model downloaded in <code>ollama</code>. This may take a bit, but we should have a way to copy it directly on your laptop.</p> <pre><code>ollama pull granite4:micro\n</code></pre> <p>If you didn't know, the supported languages with <code>granite4</code> now include:</p> <ul> <li>English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages.</li> </ul> <p>And the Capabilities also include:</p> <ul> <li>Thinking</li> <li>Summarization</li> <li>Text classification</li> <li>Text extraction</li> <li>Question-answering</li> <li>Retrieval Augmented Generation (RAG)</li> <li>Code related tasks</li> <li>Function-calling tasks</li> <li>Multilingual dialog use cases</li> <li>Fill-in-the-middle</li> <li>Long-context tasks including long document/meeting summarization, long document QA, etc.</li> </ul> <p>Next click on the <code>wrench</code> icon, and open up the settings. For now we are going to configure the global settings for <code>ollama</code> but you may want to change it in the future.</p> <p></p> <p>Click on the \"LLM\" section, and select Ollama as the LLM Provider. Also select the <code>granite4:micro</code> model. (You should be able to see all the models you have access to through <code>ollama</code> there.)</p> <p></p> <p>Click the \"Back to workspaces\" button where the wrench was. And Click \"New Workspace.\"</p> <p></p> <p>Name it something like \"learning llm\" or the name of the event we are right now, something so you know it's somewhere you are learning how to use this LLM.</p> <p></p> <p>Now we can test our connections through AnythingLLM! I like the \"Who is Batman?\" question, as a sanity check on connections and that it knows something.</p> <p></p> <p>Now you may notice that the answer is slighty different then the screen shot above. That's expected and nothing to worry about. If you have more questions about it raise your hand and one of the helpers would love to talk you about it.</p> <p>Congratulations! You have AnythingLLM running now, configured to work with <code>granite4:micro</code> and <code>ollama</code>!</p>"},{"location":"lab-5/#creating-your-own-local-rag","title":"Creating your own local RAG","text":"<p>Now that you have everything set up, lets build our own RAG. You need a document, of some sort to questions to answer against it. Lets start with something fun. As of right now, our Granite model doesn't know about the US Federal Budget in 2024, so lets ask it a question about it to verify.</p> <p>Create a new workspace, and call it whatever you want:</p> <p></p> <p>Now you have a new workspace, ask it a question like:</p> <pre><code>What was the US federal budget for 2024?\n</code></pre> <p>You should come back with something like the following, it may be different, but the gist is there.</p> <p></p> <p>Not great right? Well now we need to give it a way to look up this data, luckly, we have a backed up copy of the budget pdf here. Go ahead and save it to your local machine, and be ready to grab it.</p> <p>Note</p> <p>Granite 4 has newer data, so since this lab was created, it DOES have the 2024 data.  If you find that's the case, you can try it with the question about 2025 using the 2025 full-year budget using the link below. </p> <p>budget_fy2025.pdf</p> <p>Now spin up a New Workspace, (yes, please a new workspace, it seems that sometimes AnythingLLM has issues with adding things, so a clean environment is always easier to teach in) and call it something else.</p> <p></p> <p>Click on the \"upload a document\" to get the pdf added.</p> <p>Next we need to add it to the workspace.</p> <p></p> <p>Next click the upload or drag and drop and put the pdf in there, and then the arrow to move it to the workspace. Click Save and Embed.</p> <p>You have now added the pdf to the workspace.</p> <p>Now when the chat comes back up ask the same question, and you should see some new answers!</p> <p></p> <p>It won't be exactly what we are looking for, but it's enough to now see that the Granite model can leverage the local RAG and in turn can look things up for you. You'll need some prompt engineering to get exactly what you want but this is just the start of leveraging the AI!</p>"},{"location":"lab-6/","title":"Using Open-WebUI for a local RAG","text":""},{"location":"lab-6/#retrieval-augmented-generation-overview","title":"Retrieval-Augmented Generation Overview","text":"<p>The LLMs we're using for these labs have been trained on billions of parameters, but they haven't been trained on everything, and the smaller models have less general knowledge to work with. For example, even the latest models are trained with aged data, and they couldn't know about current events or the unique data your use-case might need.</p> <p>RAG allows the user to supplement the LLM's data with up-to-date information from external sources, like databases and documents.</p> <p>In this lab we're going to use one of the smallest IBM Granite models and show that it's answer is not complete. Then we'll add a small RAG document and allow it render a much better answer utilizing both it's internal data combined with the RAG data you give it.</p>"},{"location":"lab-6/#configuration-and-sanity-check","title":"Configuration and Sanity Check","text":"<p>Open up Open-WebUI, and you should see something like the following: </p> <p>If you see this that means Open-WebUI is installed correctly, and we can continue configuration, if not, please find a workshop TA or raise your hand we'll be there to help you ASAP.</p> <p>Next as a sanity check, run the following command to confirm you have the granite4:micro model downloaded in <code>ollama</code>. This may take a bit, but we should have a way to copy it directly on your laptop.</p> <pre><code>ollama pull granite4:micro\n</code></pre> <p>If you didn't know, the supported languages with <code>granite4:micro</code> now include:</p> <ul> <li>English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may fine-tune this Granite model for languages beyond these 12 languages.</li> </ul> <p>And the Capabilities also include:</p> <ul> <li>Summarization</li> <li>Text classification</li> <li>Text extraction</li> <li>Question-answering</li> <li>Retrieval Augmented Generation (RAG)</li> <li>Code related tasks</li> <li>Function-calling tasks</li> <li>Multilingual dialog use cases</li> <li>Fill-In-the-Middle (FIM) code completions</li> </ul> <p>Next click on the down arrow at the top and select the \"granite4:micro\" if it's not already selected.</p> <p></p> <p>Click on the \"New Chat\" icon to clear the context. Then, ask the model for: <pre><code>List all the past and current CEOs of the IBM corporation in order of their term as CEO\n</code></pre> For example: </p> <p>At first glance, the list looks pretty good. But if you know your IBM CEOs, you'll notice that it misses a few of them, and sometimes adds new names that weren't ever IBM CEOs! Retrieval Augmented Generation (RAG) allows us to provide the small LLM with a RAG document that supplements the model's missing information with a correct list, so it will generate a better answer.</p> <p>Click on the \"New Chat\" icon to clear the context.  Then download a small text file with the correct list of IBM CEOs to your Downloads folder:</p> <p>IBM.txt</p> <p>Right click on the IBM.txt URL and select \"Save Link As\" and save it as IBM in your Downloads folder.</p> <p>In your Open-WebUI browser, click on the \"+\" under the \"send a message\" prompt and then select \"Upload files\"</p> <p>Select the IBM.txt file that you just downloaded in your Downloads folder and press Open.</p> <p></p> <p>Now ask it our question about the CEOs of IBM: <pre><code>List all the past and current CEOs of the IBM corporation in order of their term as CEO\n</code></pre> The answer should now be correct. (For example, always before it forgets John Akers)</p> <p></p> <p>If you look near the bottom of the answer, you can see the RAG source that it used, along with some options you can click on, including information on the tokens per second and total tokens.</p> <p></p> <p>We can also find and download information to pdf from Wikipedia: For example: History of IBM</p> <p>On the right of the Wikipedia page, click on \"Tools\" and click on \"Download as PDF\"</p> <p>Then use this History_of_IBM.pdf as a RAG by clicking on the + and select \"History_of_IBM.pdf\" as a file from your Downloads folder.</p> <p>Next, use the Open-WebUI to ask more questions about IBM, or have it summarize the document itself.  For example: <pre><code>Write a short 150 word summary of the History_of_IBM.pdf\n</code></pre> </p> <p>Congratulations, you've completed the Open-WebUI RAG example.</p>"},{"location":"lab-7/","title":"What is Generative Computing?","text":"<p>A generative program is any computer program that contains calls to an LLM. As we will see throughout the documentation, LLMs can be incorporated into software in a wide variety of ways. Some ways of incorporating LLMs into programs tend to result in robust and performant systems, while others result in software that is brittle and error-prone.</p> <p>Generative programs are distinguished from classical programs by their use of functions that invoke generative models. These generative calls can produce many different data types \u2014 strings, booleans, structured data, code, images/video, and so on. The model(s) and software underlying generative calls can be combined and composed in certain situations and in certain ways (e.g., LoRA adapters as a special case). In addition to invoking generative calls, generative programs can invoke other functions, written in languages that do not have an LLM in their base, so that we can, for example, pass the output of a generative function into a DB retrieval system and feed the output of that into another generator. Writing generative programs is difficult because generative programs interleave deterministic and stochastic operations.</p> <p>If you would like to read more about this, please don't hesitate to take a look here.</p>"},{"location":"lab-7/#mellea","title":"Mellea","text":"<p>Mellea is a library for writing generative programs. Generative programming replaces flaky agents and brittle prompts with structured, maintainable, robust, and efficient AI workflows.</p>"},{"location":"lab-7/#features","title":"Features","text":"<ul> <li>A standard library of opinionated prompting patterns.</li> <li>Sampling strategies for inference-time scaling.</li> <li>Clean integration between verifiers and samplers.</li> <li>Batteries-included library of verifiers.</li> <li>Support for efficient checking of specialized requirements using      activated LoRAs.</li> <li>Train your own verifiers on proprietary classifier data.</li> <li>Compatible with many inference services and model families. Control cost   and quality by easily lifting and shifting workloads between:<ul> <li>inference providers</li> <li>model families</li> <li>model sizes</li> </ul> </li> <li>Easily integrate the power of LLMs into legacy code-bases (mify).</li> <li>Sketch applications by writing specifications and letting <code>mellea</code> fill in   the details (generative slots).</li> <li>Get started by decomposing your large unwieldy prompts into structured and maintainable Mellea problems.</li> </ul>"},{"location":"lab-7/#lets-setup-mellea-to-work-locally","title":"Let's setup Mellea to work locally","text":"<p>Open up a terminal, and run the following commands: <pre><code>python3.11 -m venv venv\nsource venv/bin/activate\npip install mellea\n</code></pre></p> <p>Note</p> <p>If you see something about the Rust compiler, please confirm you are using python3.11, or python3.12 anything above that has a Rust dependency.</p> <ol> <li> <p>Start python:</p> <pre><code>python\n</code></pre> </li> <li> <p>Run a simple Mellea session:</p> <p><pre><code>import mellea\n\nm = mellea.start_session()\nprint(m.chat(\"tell me some fun trivia about IBM and the early history of AI.\").content)\n</code></pre> You can either add this to a file like <code>main.py</code> or run it in the python REPL, if you get output you are set up to dig deeper with Mellea.</p> </li> </ol>"},{"location":"lab-7/#simple-email-examples","title":"Simple email examples","text":"<p>Note</p> <p>The following work should be done via a text editor, there should be a couple installed on your laptop, if you aren't sure raise your hand and a helper will help you out.</p> <ol> <li> <p>Let's leverage Mellea to do some email generation for us, the first example is a simple example:</p> <pre><code>import mellea\nm = mellea.start_session()\n\nemail = m.instruct(\"Write an email inviting interns to an office party at 3:30pm.\")\nprint(str(email))\n</code></pre> </li> <li> <p>As you can see, it outputs a standard email with only a couple lines of code, lets expand on this:</p> <p><pre><code>import mellea\nm = mellea.start_session()\n\ndef write_email(m: mellea.MelleaSession, name: str, notes: str) -&gt; str:\n    email = m.instruct(\n        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n        user_variables={\"name\": name, \"notes\": notes},\n    )\n    return email.value  # str(email) also works.\n\n\nprint(\n    write_email(\n        m,\n        \"Olivia\",\n        \"Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery.\",\n    )\n)\n</code></pre> With this more advance example we now have the ability to customize the email to be more directed and personalized for the recipient. But this is just a more programmatic prompt engineering, lets see where Mellea really shines.</p> </li> </ol>"},{"location":"lab-7/#simple-email-with-boundaries-and-requirements","title":"Simple email with boundaries and requirements","text":"<ol> <li> <p>The first step with the power of Mellea, is adding requirements to something like this email, take a look at this first     example:</p> <pre><code>import mellea\nm = mellea.start_session()\n\ndef write_email_with_requirements(\n    m: mellea.MelleaSession, name: str, notes: str\n) -&gt; str:\n    email = m.instruct(\n        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n        requirements=[\n            \"The email should have a salutation\",\n            \"Use only lower-case letters\",\n        ],\n        user_variables={\"name\": name, \"notes\": notes},\n    )\n    return str(email)\n\n\nprint(\n    write_email_with_requirements(\n        m,\n        name=\"Olivia\",\n        notes=\"Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery.\",\n    )\n)\n</code></pre> <p>As you can see with this output now, you force the Mellea framework to start checking itself to create what you need. Imagine this possibility, now you can start making sure your LLMs only generate things that you want. Test this theory by changing from \"only lower-case\" to \"only upper-case\" and see that it will follow your instructions.</p> <p>Pretty neat eh? Lets go even deeper.</p> <p>Let's create an email with some sampling and have Mellea find the best option for what we are looking for: We add two requirements to the instruction which will be added to the model request. But we don't check yet if these requirements are satisfied, we add a strategy for validating the requirements.</p> </li> <li> <p>This sampling strategy (<code>RejectionSamplingStrategy()</code>) checks if all requirements are met and if any requirement fails, the sampling strategy will sample a new email from the LLM.</p> <pre><code>import mellea\nm = mellea.start_session()\n\nfrom mellea.stdlib.sampling import RejectionSamplingStrategy\n\n\ndef write_email_with_strategy(m: mellea.MelleaSession, name: str, notes: str) -&gt; str:\n    email_candidate = m.instruct(\n        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n        requirements=[\n            \"The email should have a salutation\",\n            \"Use only lower-case letters\",\n        ],\n        strategy=RejectionSamplingStrategy(loop_budget=5),\n        user_variables={\"name\": name, \"notes\": notes},\n        return_sampling_results=True,\n    )\n    if email_candidate.success:\n        return str(email_candidate.result)\n    else:\n        print(\"Expect sub-par result.\")\n        return email_candidate.sample_generations[0].value\n\n\nprint(\n    write_email_with_strategy(\n        m,\n        \"Olivia\",\n        \"Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery.\",\n    )\n)\n</code></pre> <p>You might notice it fails with the above example, because the name \"Olivia\" has an upper-case letter in it.  Remove the <code>\"Use only lower-case letters\",</code> line, and it should pass on the first re-run. This brings up some interesting opportunities, so make sure that the writing you expect is within the boundaries and it'll keep trying till it gets it right.</p> </li> </ol>"},{"location":"lab-7/#instruct-validate-repair","title":"Instruct Validate Repair","text":"<p>The first <code>instruct-validate-repair</code> pattern is as follows:</p> <pre><code>import mellea\nfrom mellea.stdlib.requirement import req, check, simple_validate\nfrom mellea.stdlib.sampling import RejectionSamplingStrategy\n\ndef write_email(m: mellea.MelleaSession, name: str, notes: str) -&gt; str:\n    email_candidate = m.instruct(\n        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n        requirements=[\n            req(\"The email should have a salutation\"),  # == r1\n            req(\n                \"Use only lower-case letters\",\n                validation_fn=simple_validate(lambda x: x.lower() == x),\n            ),  # == r2\n            check(\"Do not mention purple elephants.\"),  # == r3\n        ],\n        strategy=RejectionSamplingStrategy(loop_budget=5),\n        user_variables={\"name\": name, \"notes\": notes},\n        return_sampling_results=True,\n    )\n    if email_candidate.success:\n        return str(email_candidate.result)\n    else:\n        return email_candidate.sample_generations[0].value\n\nm = mellea.start_session()\nprint(\n    write_email(\n        m,\n        \"Olivia\",\n        \"Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery.\",\n    )\n)\n</code></pre> <p>Most of this should look familiar by now, but the <code>validation_fn</code> and <code>check</code> should be new.</p> <p>We create 3 requirements:</p> <ul> <li>First requirement (r1) will be validated by LLM-as-a-judge on the output of the instruction. This is the default behavior.</li> <li>Second requirement (r2) uses a function that takes the output of a sampling step and returns a boolean value indicating successful or unsuccessful validation. While the validation_fn parameter requires to run validation on the full session context, Mellea provides a wrapper for simpler validation functions (simple_validate(fn: Callable[[str], bool])) that take the output string and return a boolean as seen in this case.</li> <li>Third requirement is a check(). Checks are only used for validation, not for generation. Checks aim to avoid the \"do not think about B\" effect that often primes models (and humans) to do the opposite and \"think\" about B.</li> <li>We also demonstrate in the m = mellea.start_session() how you can specify a different Ollama model, in case you want to try something other than Mellea's ibm/granite4:micro default.</li> </ul> <p>Run this in your local instance, and you'll see it working, and ideally no purple elephants! :)</p> <p>Hopefully you felt like you've learned a bunch about AI and engaging with our open source models through this journey. Never hesitate to give us any feedback, and remember all of this stuff is free, open source, Apache 2 licensed, and designed to work in the Enterprise ecosystem. Thanks for reading and joining us!</p>"},{"location":"pre-work/","title":"Workshop Pre-Work","text":"<p>These are the required applications and general installation notes for this workshop.</p> <p>Ollama and Python are required for this workshop, but you can choose an IDE and GUI interface from the options provided. If you don't know what to select, just go with the recommended options!</p> <p>Remember, you can always ask the teacher for help if you get stuck on any step!</p>"},{"location":"pre-work/#required-software","title":"Required Software","text":"<ul> <li>Python</li> <li>Ollama - Allows you to locally host an LLM model on your computer.</li> <li>Open WebUI</li> </ul>"},{"location":"pre-work/#installing-python","title":"Installing Python","text":"<p>There are multiple ways to install Python, you can follow their beginner's guide based on your operating system.</p>"},{"location":"pre-work/#using-homebrew-mac","title":"Using Homebrew (Mac)","text":"<p>Install Homebrew using the following command:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Then, install Python via <code>brew</code>:</p> <pre><code>brew install python@3.11\n</code></pre> <p>Please confirm that your <code>python --version</code> is at least <code>3.11+</code> for the best experience.</p> <p>Note</p> <p>python 3.11 and 3.12 work best.  Python 3.13 has trouble with Open-WebUI at the moment.</p>"},{"location":"pre-work/#installing-ollama","title":"Installing Ollama","text":"<p>Most users can simply download Ollama from its website.</p>"},{"location":"pre-work/#using-homebrew-mac_1","title":"Using Homebrew (Mac)","text":"<p>Install Homebrew using the following command:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Then, install Ollama via <code>brew</code>:</p> <pre><code>brew install ollama\n</code></pre> <p>Note</p> <p>You can save time by starting the model download used for the lab in the background by running <code>ollama pull granite4:micro</code> in its own terminal. You might have to run <code>ollama serve</code> first depending on how you installed it.</p>"},{"location":"pre-work/#installing-open-webui","title":"Installing Open-WebUI","text":"<p>Assuming you've set up Python above, use the following commands to install Open-WebUI:</p> <pre><code>cd ~\nmkdir openweb-ui\ncd openweb-ui\npython3.11 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip install open-webui\nopen-webui serve\n</code></pre>"},{"location":"pre-work/#conclusion","title":"Conclusion","text":"<p>Now that you have all of the tools you need, head over to Lab 1 if you have AnythingLLM or Lab 1.5 for Open-WebUI.</p>"},{"location":"resources/MKDOCS/","title":"mkdocs examples","text":"<p>This page includes a few neat tricks that you can do with <code>mkdocs</code>. For a complete list of examples visit the mkdocs documentation.</p>"},{"location":"resources/MKDOCS/#code","title":"Code","text":"<pre><code>print(\"hello world!\")\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-line-numbers","title":"Code with line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-highlights","title":"Code with highlights","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-tabs","title":"Code with tabs","text":"Tab HeaderAnother Tab Header <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"resources/MKDOCS/#more-tabs","title":"More tabs","text":"WindowsMacOSLinux <p>If on windows download the <code>Win32.zip</code> file and install it.</p> <p>Run <code>brew install foo</code>.</p> <p>Run <code>apt-get install foo</code>.</p>"},{"location":"resources/MKDOCS/#checklists","title":"Checklists","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> In hac habitasse platea dictumst</li> </ul>"},{"location":"resources/MKDOCS/#add-a-button","title":"Add a button","text":"<p>Launch the lab</p> <p>Visit IBM Developer</p> <p>Sign up! </p>"},{"location":"resources/MKDOCS/#call-outs","title":"Call outs","text":"<p>Tip</p> <p>You can use <code>note</code>, <code>abstract</code>, <code>info</code>, <code>tip</code>, <code>success</code>, <code>question</code> <code>warning</code>, <code>failure</code>, <code>danger</code>, <code>bug</code>, <code>quote</code> or <code>example</code>.</p> <p>Note</p> <p>A note.</p> <p>Abstract</p> <p>An abstract.</p> <p>Info</p> <p>Some info.</p> <p>Success</p> <p>A success.</p> <p>Question</p> <p>A question.</p> <p>Warning</p> <p>A warning.</p> <p>Danger</p> <p>A danger.</p> <p>Example</p> <p>A example.</p> <p>Bug</p> <p>A bug.</p>"},{"location":"resources/MKDOCS/#call-outs-with-code","title":"Call outs with code","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.</p>"},{"location":"resources/MKDOCS/#formatting","title":"Formatting","text":"<p>In addition to the usual italics, and bold there is now support for:</p> <ul> <li>highlighted</li> <li>underlined</li> <li>strike-through</li> </ul>"},{"location":"resources/MKDOCS/#tables","title":"Tables","text":"OS or Application Username Password Windows VM <code>Administrator</code> <code>foo</code> Linux VM <code>root</code> <code>bar</code>"},{"location":"resources/MKDOCS/#emojis","title":"Emojis","text":"<p>Yes, these work.  </p>"},{"location":"resources/MKDOCS/#images","title":"Images","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/MKDOCS/#right-align-image","title":"right align image","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/RESOURCES/","title":"Additional resources","text":""},{"location":"resources/RESOURCES/#ibm-demos","title":"IBM Demos","text":"<ul> <li>Collection: InfoSphere Information Server</li> <li>Tutorial: Transforming your data with IBM DataStage</li> </ul>"},{"location":"resources/RESOURCES/#redbooks","title":"Redbooks","text":"<ul> <li>IBM InfoSphere DataStage Data Flow and Job Design</li> <li>InfoSphere DataStage Parallel Framework Standard Practices</li> </ul>"},{"location":"resources/RESOURCES/#videos","title":"Videos","text":"<ul> <li>Video: Postal codes and part numbers (DataStage)</li> <li>Video: Find relationships between sales, employees, and customers (Information Analyzer)</li> <li>Video: Clean and analyze data (Governance Catalog)</li> </ul>"}]}